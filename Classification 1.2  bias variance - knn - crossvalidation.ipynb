{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"scrolled":true,"id":"wVeTFMhlNC55","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":885},"executionInfo":{"status":"ok","timestamp":1666618065613,"user_tz":-180,"elapsed":27692,"user":{"displayName":"Giorgos Siolas","userId":"10127542075805046236"}},"outputId":"cf8016d5-9c52-41b0-a7d3-e17ee73de74c"},"source":["!pip install --upgrade pip #upgrade pip package installer\n","!pip install scikit-learn --upgrade #upgrade scikit-learn package\n","!pip install numpy --upgrade #upgrade numpy package\n","!pip install --upgrade matplotlib # Κάνουμε update την matplotlib"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Collecting pip\n","  Downloading pip-22.3-py3-none-any.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 5.3 MB/s \n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 21.1.3\n","    Uninstalling pip-21.1.3:\n","      Successfully uninstalled pip-21.1.3\n","Successfully installed pip-22.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n","Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n","Collecting matplotlib\n","  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (21.3)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n","Collecting fonttools>=4.22.0\n","  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n","Installing collected packages: fonttools, matplotlib\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.2.2\n","    Uninstalling matplotlib-3.2.2:\n","      Successfully uninstalled matplotlib-3.2.2\n","Successfully installed fonttools-4.38.0 matplotlib-3.5.3\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["matplotlib","mpl_toolkits"]}}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"8zR-sydoN4o5"},"source":["# Παραμετρικοί και μη-παραμετρικοί ταξινομητές, bias - variance trade-off\n","\n","![](https://miro.medium.com/max/500/1*1jV03DHuQeVkdzUKrwmX4w.png)\n","\n","O Gaussian Naive Bayes είναι ένας παραμετρικός ταξινομητής. Οι παραμετρικοί ταξινομητές κάνουν κάποια υπόθεση για την κατανομή (των χαρακτηριστικών) των δεδομένων και την προσδιορίζουν μέσω παραμέτρων. Στην περίπτωση του Gaussian Naive Bayes, η υπόθεση είναι η κανονική κατανομή και οι παράμετροι είναι τα $μ$ και $σ^2$ των χαρακτηριστικών. Αντίθετα, οι μη-παραμετρικές μέθοδοι δεν κάνουν καμία υπόθεση για την κατανομή των δεδομένων. Προσοχή: και οι μη-παραμετρικοί ταξινομητές έχουν παραμέτρους (και πάρα πολλές σε ορισμένες περιπτώσεις, τα βάρη των νευρωνικών για παράδειγμα) που επηρ\n","εάζουν τη λειτουργία τους αλλά δεν σχετίζονται με κάποια υπόθεση κατανομής για τα δεδομένα. \n","\n","Σε γενικές γραμμές οι παραμετρικοί ταξινομητές είναι απλούστεροι, ταχύτεροι στις φάσεις train/test και χρειάζονται λιγότερα δεδομένα εκπαίδευσης. Από την άλλη, έχουν γενικά μικρότερη χωρητικότητα (capacity), δηλαδή μπορούν να διαχωρίσουν τις κλάσεις σε προβλήματα σχετικά μικρότερων διαστάσεων ενώ η απαίτηση τα πραγματικά δεδομένα να ακολουθούν μια ακριβή κατανομή είναι πολύ ισχυρή και δεν επαληθεύεται πρακτικά. Αντιστρόφως, οι μη παραμετρικοί ταξινομητές είναι πιο αργοί στην εκπαίδευση, έχουν γενικά μεγαλύτερες απαιτήσεις χώρου/μνήμης και χρειάζονται περισσότερα δεδομένα αλλά έχουν μεγαλύτερη χωρητικότητα, μπορούν να μάθουν δυσκολότερα προβλήματα και να έχουν καλύτερη απόδοση σε μεγαλύτερα datasets. \n","\n","Οι μη παραμετρικοί ταξινομητές μπορούν να εμφανίσουν επίσης εντονότερα το πρόβλημα της υπερεκπαίδευσης (overfitting), δηλαδή να προσαρμοστούν υπερβολικά στα δεδομένα εκπαίδευσης και να μειωθεί η ικανότητα γενίκευσης (generalisation) τους σε νέα δείγματα. Γενικά στη φάση της εκπαίδευσης προσπαθούμε να επιτύχουμε μια καλή ισορροπία μεταξυ της απόκλισης (bias) και της διακύμανσης (variance) από τις πραγματικές τιμές.\n","\n","![Bias-Variance trade off](https://cdn-images-1.medium.com/proxy/1*e4Kn-_M_KN2bw-e6kevywA.png \"Bias-Variance trade off\")\n","\n","\n","Ένα παράδειγμα μη-παραμετρικού ταξινομητή που θα εξετάσουμε είναι ο kNN (k-Nearest-Neighbours). "]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"4ZdyhJaEN4o8"},"source":["# k Nearest Neighbors Classifier (kNN)\n","O kNN είναι ένας μη παραμετρικός ταξινομητής βασισμένος σε παραδείγματα (instance-based). Η αρχή λειτουργίας του είναι πολύ απλή. Για ένα νέο δείγμα προς ταξινόμηση, πρώτα υπολογίζουμε τους k πλησιέστερους γείτονές του (στον ν-διάστατο χώρο των χαρακτηριστικών εισόδου) με βάση κάποια συνάρτηση απόστασης, συνήθως ευκλείδεια\n","$$d(x, x') = \\sqrt{\\left(x_1 - x'_1 \\right)^2 + \\left(x_2 - x'_2 \\right)^2 + \\dotsc + \\left(x_n - x'_n \\right)^2}$$\n","Η κλάση του νέου δείγματος θα είναι η κλάση της πλειοψηφίας των k γειτόνων (διαλέγουμε k περιττό γενικά), είτε απλά υπολογισμένη (άθροισμα) είτε (αντίστροφα) ζυγισμένη με βάση την απόσταση του κάθε γείτονα. \n","\n","Ο kNN δεν έχει πρακτικά φάση εκπαίδευσης. Ωστόσο, για να ταξινομήσουμε ένα νέο δείγμα στην φάση test,  πρέπει να συγκρίνουμε την απόστασή του με κάθε δείγμα του train set. Αυτό σημαίνει ότι για την ταξινόμηση είναι απαραίτητα όλα τα δείγματα εκπαίδευσης (εξού και η ονομασία \"instance-based\", ενώ στον Naive Bayes χρειαζόμαστε μόνο τις παραμέτρους $μ$ και $σ^2$). Αυτό σημαίνει ότι ο kNN είναι πιο απαιτητικός και σε χώρο (αποθήκευση όλων των δειγμάτων) και σε χρόνο (υπολογισμός όλων των αποστάσεων για κάθε νέο δείγμα).\n","# Υπερπαράμετρος k\n","Το k της γειτονιάς του kNN είναι μια υπερπαράμετρος του ταξινομητή. Μια άλλη υπερπαράμετρος για παράδειγμα είναι η συνάρτηση της απόστασης. Οι υπερπαράμετροι είναι επιλογές που γίνονται από τον σχεδιαστή του συστήματος και δεν μπορούμε να ξέρουμε τις βέλτιστες τιμές τους αν πρώτα δεν τις αξιολογήσουμε εμπειρικά σε δεδομένα.  Ένα άλλο παράδειγμα υπερπαραμέτρου είναι ο αριθμός των κρυφών νευρώνων σε ένα MLP. Στην περίπτωση του kNN το k ελέγχει το trade-off μεταξύ μεταξύ απόκλισης και διακύμνανσης.\n","\n","Έαν θέσουμε μικρό k, πχ k=1 παίρνουμε ένα ταξινομητή με υψηλή διακύμανση και χαμηλή απόκληση. Ο ταξινομητής τείνει να αγνοεί τη συνολική κατανομή και αποφασίζει μόνο από το κοντινότερο δείγμα. Στην περίπτωση k=1 το σύνορο απόφασης (decision boundary) περνά από τις μεσοκάθετους γειτονικών δειγμάτων διαφορετικής κλάσης. \n","\n","![kNN k=1](https://i.stack.imgur.com/UG81y.png \"kNN with k=1\")\n","\n","Αν διαλέξουμε μεγαλύτερο k, φτιάχνουμε ένα ταξινομητή με χαμηλότερη διακύμανση και υψηλότερη απόκλιση. Θα ταξινομίσει λάθος περισσότερα αποκλίνοντα δείγματα (outliers) αλλά θα σέβεται περισσότερο τη συνολική κατανομή.\n","![kNN k=20](https://i.stack.imgur.com/FZITG.png \"kNN with k=20\")\n","\n","# Το Iris dataset\n","Για να μελετήσουμε τον kNN, θα φορτώσουμε το dataset [\"Iris\"](https://archive.ics.uci.edu/ml/datasets/Iris). Το Iris είναι το διασημότερο dataset στο Machine Learning. Το χρησιμοποίησε πρώτος το 1936 ο διάσημος στατιστικός [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) και αποτελείται από 50 παρατηρήσεις για κάθε είδος του άνθους Iris (Iris setosa, Iris virginica και Iris versicolor). Για κάθε δείγμα μετρήθηκαν 4 χαρακτηριστικά: το πλάτος και το μήκος των πετάλων και των σεπάλων.\n","\n","![Iris petals & sepals](https://www.integratedots.com/wp-content/uploads/2019/06/iris_petal-sepal-e1560211020463.png \"Iris petals & sepals\")\n","\n","Σημειώστε ότι με το Iris και τρεις κατηγορίες και άρα έχουμε multiclass και όχι binary classification. Εισάγουμε το Iris στο notebook."]},{"cell_type":"code","metadata":{"id":"z1uirGVkN4o-","executionInfo":{"status":"ok","timestamp":1666618066400,"user_tz":-180,"elapsed":798,"user":{"displayName":"Giorgos Siolas","userId":"10127542075805046236"}}},"source":["# Load Iris and organize our data\n","from sklearn.datasets import load_iris\n","data = load_iris()\n","label_names = data['target_names']\n","labels = data['target']\n","feature_names = data['feature_names']\n","features = data['data']"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSoLp7_UN4pB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666618066400,"user_tz":-180,"elapsed":5,"user":{"displayName":"Giorgos Siolas","userId":"10127542075805046236"}},"outputId":"0a1e1500-f632-47f4-f5f4-480bcbf78c95"},"source":["# Ας τυπώσουμε τα ονόματα των χαρκτηριστικών\n","print(feature_names)\n","print(label_names)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n","['setosa' 'versicolor' 'virginica']\n"]}]},{"cell_type":"code","metadata":{"id":"HIZ_g3-MN4pG","executionInfo":{"status":"ok","timestamp":1666618571186,"user_tz":-180,"elapsed":586,"user":{"displayName":"Giorgos Siolas","userId":"10127542075805046236"}}},"source":["# Χρησιμοποιούμε τη γνωστή train_test_split για να διαχωρίσουμε σε train και test set\n","# το (int) όρισμα \"random_state\" είναι το seed της γεννήτριας τυχαίων αριθμών\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.40, random_state=78)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m0D806YqN4pJ"},"source":["Διαλέγουμε τυχαία τιμή 5 για την υπερπαράμετρο k, εκπαιδεύουμε ένα kNN classifier και υπολογίζουμε την πιστότητα του στο Iris, στο προηγούμενο train/test split"]},{"cell_type":"code","metadata":{"id":"X1vVA6_zN4pK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666618572803,"user_tz":-180,"elapsed":6,"user":{"displayName":"Giorgos Siolas","userId":"10127542075805046236"}},"outputId":"bad667dc-3447-45fb-bac4-639b65c689c7"},"source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","knn = KNeighborsClassifier(n_neighbors=1)\n","knn.fit(X_train, y_train)\n","pred = knn.predict(X_test)\n","print(accuracy_score(y_test, pred))"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9666666666666667\n"]}]},{"cell_type":"markdown","metadata":{"id":"zc4Mx50nN4pO"},"source":["# Ρύθμιση υπερπαραμέτρων με διασταυρούμενη επικύρωση (Cross Validation)\n","\n","![](https://cdn.sstatic.net/Sites/stats/Img/apple-touch-icon.png?v=8698dcfb5c72)\n","\n","[Cross Validated Website](https://stats.stackexchange.com/)\n","\n","Ένας σωστός τρόπος για να βρούμε τη βέλτιστη τιμή του k, να πραγματοποιήσουμε δηλαδή επικύρωση του μοντέλου, είναι ο ακόλουθος. Για k=1 μέχρι k=κάποιο n, κάνουμε fit τον ταξινομητή στο train set και μετράμε την απόδοση στο test set. Ο ταξινομητής με k που δίνει το μικρότερο σφάλμα ταξιμόνόμησης σύμφωνα με κάποιο κριτήριο (εδώ η πιστότητα) στο test set θα είναι ο βέλτιστος. Όμως, αν ακολουθήσουμε αυτή τη στρατηγική, ουσιαστικά κάνουμε υπερεκπαίδευση, καθώς χρησιμοποιούμε το test set ως training set, δηλαδή βελτιστοποιούμε κάποιο κριτήριο σφάλματος πάνω στο test set. Αυτό μπορεί να είναι επιβλαβές για την ικανότητα γενίκευσης του ταξινομητή: το test set χρησιμεύει μόνο για την τελική εκτίμηση της απόδοσης του ταξινομητή.\n","\n","Για να ακολουθήσουμε σωστά το πρωτόκολλο, αυτό που πρέπει να κάνουμε είναι να χρησιμοποιήσουμε μόνο το πραγματικό training set για να διαλέξουμε τις βέλτιστες υπερπαραμέτρους. Θα μπορούσαμε να κρατήσουμε ένα ποσοστό δειγμάτων ως σύνολο επικύρωσης (validation set πχ άλλο ένα 1/3) του training set και να ακολουθήσουμε την προηγούμενη διαδικασία: εκπαίδευση στο 1/3 training set, επικύρωση σε 1/3 και τελικά αξιολόγηση στο 1/3 data set. Ωστόσο αυτή η μεθοδολογία \"αχρηστεύει\" μεγάλο μέρος του dataset (τα 2/3) ως προς την εκπαίδευση του ταξινομητή. Πρακτικά λοιπόν, προτιμούμε να χρησιμοποιούμε τη μέθοδο της διασταυρούμενης επικύρωσης (Cross Validation).\n","\n","Στο Cross Validation αρχικά χωρίζουμε το training set σε έναν αριθμό \"πτυχών\" (folds). Συνηθισμένες τιμές είναι το 5 και το 10 (5-fold και 10-fold CV). Στη συνέχεια, για κάθε k-fold (άσχετο από το k του kNN), θεωρούμε ότι τα k μείον 1 folds είναι training set και ότι το fold που αφήσαμε έξω είναι το test set. Υπολογίζουμε τη μετρική σφάλματός μας στο test set που ορίζει το fold. Επαναλαμβάνουμε τη διαδικασία για τα k folds για κάθε τιμή των υπερπαραμέτρων και υπολογίζουμε τη μέση τιμή της μετρικής του σφάλματος. Με αυτό τον τρόπο, αφενός είμαστε αμερόληπτοι στην αξιολόγηση αφήνοντας τελείως έξω το test set και αφετέρου χρησιμοποιούμε αποτελεσματικά τα δεδομένα εκπαίδευσης: τα χρησιμοποιούμε όλα και παίρνοντας τη μέση τιμή εξαλείφουμε πιθανές ανωμαλίες στα δεδομένα.\n","\n","![Cross validation](https://sebastianraschka.com/images/faq/evaluate-a-model/k-fold.png \"Cross Validation\")\n","\n","To Scikit Learn έχει συναρτήσεις για να κάνει αυτόματα cross validation (να ορίζει folds και να  υπολογίζει τιμές και μέσους όρους). Θα κάνουμε 5 fold cross validation για να υπολογίσουμε το βέλτιστο k του kNN στο Iris."]},{"cell_type":"code","metadata":{"id":"3q1krR5RN4pO","executionInfo":{"status":"ok","timestamp":1666618164655,"user_tz":-180,"elapsed":682,"user":{"displayName":"Giorgos Siolas","userId":"10127542075805046236"}}},"source":["from sklearn.model_selection import cross_val_score\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# φτιάχνουμε μια λίστα από το 1 έως το 50\n","myList = list(range(1,50))\n","# Κρατάμε μόνο τα περιττά k\n","neighbors = list(filter(lambda x: x % 2 != 0, myList))\n","# empty list that will hold cv scores\n","cv_scores = []\n","# perform 5-fold cross validation\n","for k in neighbors:\n","    knn = KNeighborsClassifier(n_neighbors=k)\n","    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n","    cv_scores.append(scores.mean())"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Usr5iw5N4pR"},"source":["Θα τυπώσουμε την τιμή του σφάλματος σε γραφική παράσταση ως συνάρτηση του k για το training set, και την τιμή της πιστότητας στο test set για το βέλτιστο k (υπολογισμένο στο training set)"]},{"cell_type":"code","metadata":{"id":"U7izRFzIPZHU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666618174068,"user_tz":-180,"elapsed":1821,"user":{"displayName":"Giorgos Siolas","userId":"10127542075805046236"}},"outputId":"c173c13f-b325-474e-bb6e-a3a4ef702d2a"},"source":["# Κάνουμε update την matplotlib, ίσως χρειαστεί restart του runtime\n","!pip install --upgrade matplotlib"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.5.3)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (21.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (4.38.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","metadata":{"id":"TiB323l-N4pS","colab":{"base_uri":"https://localhost:8080/","height":450},"executionInfo":{"status":"ok","timestamp":1666618579852,"user_tz":-180,"elapsed":357,"user":{"displayName":"Giorgos Siolas","userId":"10127542075805046236"}},"outputId":"88bcb625-f922-4352-decc-bc86f4c330ab"},"source":["# Κάνουμε import την matplotplib\n","import matplotlib.pyplot as plt\n","\n","# το σφάλμα είναι το αντίστροφο της πιστότητας\n","mean_error = [1 - x for x in cv_scores]\n","\n","# plot misclassification error vs k\n","plt.plot(neighbors, mean_error)\n","plt.xlabel('Number of Neighbors K')\n","plt.ylabel('Misclassification Error')\n","plt.show()\n","\n","# determining best k\n","optimal_k = neighbors[mean_error.index(min(mean_error))]\n","print(\"The optimal number of neighbors (calculated in the training set) is %d\" % optimal_k)\n","\n","# για το optimal k παίρνουμε και τα αποτέλεσματα στο test set\n","knn = KNeighborsClassifier(n_neighbors = optimal_k)\n","knn.fit(X_train, y_train)\n","pred = knn.predict(X_test)\n","print(\"\\nOptimal accuracy on the test set is\", accuracy_score(y_test, pred), \"with k=\", optimal_k)"],"execution_count":11,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2092\u001b[0m         \u001b[0mratio\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mratio\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlogical\u001b[0m \u001b[0mto\u001b[0m \u001b[0mphysical\u001b[0m \u001b[0mpixels\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2094\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2095\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m_get_renderer\u001b[0;34m(figure, print_method)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     \u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_without_rendering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1560\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1561\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_non_interactive_terminal_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m     \"\"\"\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mMetadata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPNG\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbytes\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlatin\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mencodable\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mAccording\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPNG\u001b[0m \u001b[0mspecification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mshorter\u001b[0m \u001b[0mthan\u001b[0m \u001b[0;36m79\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0mchars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name '_png' from 'matplotlib' (/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py)"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["The optimal number of neighbors (calculated in the training set) is 5\n","\n","Optimal accuracy on the test set is 0.9833333333333333 with k= 5\n"]}]},{"cell_type":"markdown","metadata":{"id":"E_vqjGz4HYFm"},"source":["👉 Δεν είναι απαραίτητο ότι η καλύτερη τιμή της υπερπαραμέτρου που βρίσκουμε θα έχει τη βέλτιστη απόδοση στο test set, αλλά είμαστε σίγουροι ότι θα είναι μια πολύ καλή τιμή - κοντά στη βέλτιστη."]},{"cell_type":"markdown","metadata":{"id":"T7MDvQnDDBo4"},"source":["# Η διαδίκασία επικύρωσης μοντέλων\n","\n","## train - validation - test\n","\n","Θεωρούμε ότι έχουμε αρκετά (πολλά) δείγματα. Αν δεν έχουμε πολλά δείγματα δεν μπορούμε να κάνουμε μηχανική μάθηση.\n","\n","![](https://scikit-learn.org/stable/_static/ml_map.png) [interactive map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n","\n","Αν ανακατέψουμε τη σειρά των δειγμάτων και χωρίσουμε το dataset σε σχετικά μεγάλα κομάτια (2 ή 3), πιστεύουμε (ελπίζουμε) με αρκετή βεβαιότητα ότι η κατανομή των ετικετών και των τιμών των χαρακτηριστικών σε κάθε κομάτι θα είναι η ίδια με του συνολικού dataset.\n","\n","Διαχωρίζουμε άπαξ τα δεδομένα σε δεδομένα εκπαίδευσης (train set) και ελέγχου (test set). Γενικά το train set είναι μεγαλύτερο από το test set πχ 70%-30% (αυτό εξαρτάται από τα δεδομένα).\n","\n","Το test set θα χρησιμοποιηθεί μόνο στο τέλος για την εκτίμηση της γενίκευσης, δεν το αγκίζουμε στην εκπαίδευση.\n","\n","Το train set μπορούμε να το χρησιμποιήσουμε ολόκληρο για cross validation (οπότε έχουμε δύο σύνολα, το train και το test - Σχήμα Α) για την εύρεση των βέλτιστων υπερπαραμέτρων. Είναι το συνηθέστερο γιατί χρησιμοποιεί όλα τα δεδομένα του train. \n","\n","Αφού καταλήξουμε στις βέλτιστες υπερπαραμέτρους χρησιμοποιούμε ολόκληρο το train set για την εκπαίδευση (if applicable).\n","\n","Σε περιπτώσεις που δεν θα κάνουμε crossvalidation για εύρεση υπερπαραμέτρων (πχ deep learning) χωρίζουμε εκ νέου το train set σε train set και validation set (Σχήμα Β).\n","\n","![](https://upload.wikimedia.org/wikipedia/commons/b/bb/ML_dataset_training_validation_test_sets.png)\n","\n","Ποια είναι διαφορά train και validation στο Β:\n","- κατά την εκπαίδευση το σφάλμα στο train set χρησιμοποιείται για την επίβλεψη και διόρθωση των βαρών του ταξινομητή\n","- το σφάλμα στο validation χρησιμοποιείται για να έχουμε μια εκτίμηση της γενίκευσης αλλά δεν χρησιμοποιείται για την διόρθωση των βαρών. Αν το σφάλμα στο train set μικραίνει και στο validation set μεγαλώνει έχουμε αρχίσει να έχουμε υπερεκπαίδευση.\n","\n","## Άλλα σχήματα Cross Validation\n","\n","### Εξαντλητικά σχήματα\n","\n","#### Leave-one-out Cross Validation\n","![](https://i2.wp.com/neptune.ai/wp-content/uploads/Cross-validation-leave-one-out.jpg?resize=564%2C440&ssl=1)\n","\n","H πιο ακριβής μέθοδος cross validation αλλά και η πιο ακριβή (χρονοβόρα). Υπάρχει και η παραλλαγή Leave-p-out cross-validation.\n","\n","### Nested Cross Validation\n","\n","Χρησιμοποιείται για να κάνουμε ταυτόχρονα εύρεση βέλτιστων υπερπαραμέτρων και εκτίμηση της γενίκευσης.\n","\n","#### k*l-fold cross-validation\n","![image.png](https://i.stack.imgur.com/vh1sZ.png)\n","\n","Κάνουμε k εξωτερικά folds, το 1 fold είναι για εκτίμηση της γενίκευσης και τα k-1 για εκπαίδευση. Στο σύνολο train των k-1 κάνουμε l-fold crossvalidation για εύρεση των βέλτιστων υπερπαραμέτρων του ταξινομητή.\n","\n","Ο μέσος όρος της γενίκευσης στα k folds του train set είναι μια εκτίμηση χωρίς προκατάληωη (unbiased, σα να κάνουμε marginilize την επίδραση των υπερπαραμέτρων) της γενίκευσης στο test set.\n","\n","H Nested Cross Validation χρησιμεύει για την unbiased εκτίμηση της γενίκευσης στο test set αλλά όχι για την επιλογή υπερπαραμέτρων (βλέπε [εδώ](https://weina.me/nested-cross-validation/)).\n","\n"]}]}